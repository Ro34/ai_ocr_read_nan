# Online VLM API configuration
# Copy this file to `.env` and fill in your real values.

# Required: API endpoint
# - For generic providers supporting image captioning via file/base64:
#   VLM_API_URL=https://your-api.example.com/v1/caption
# - For SiliconFlow (OpenAI-compatible chat-completions with image):
#   VLM_API_URL=https://api.siliconflow.cn/v1/chat/completions
VLM_API_URL=https://your-api.example.com/v1/caption

# Optional auth: the header key and key prefix (e.g. "Bearer ")
VLM_API_KEY=
VLM_API_AUTH_HEADER=Authorization
VLM_API_KEY_PREFIX=Bearer 

# How to send the image: "multipart" or "base64"
VLM_API_MODE=multipart
# Field name for the image in API request
VLM_API_IMAGE_FIELD=file

# Which JSON key contains the description in API response
VLM_API_DESC_KEY=description

# Optional model name for the API (depends on your provider)
VLM_API_MODEL=

# Timeout seconds for HTTP requests
VLM_API_TIMEOUT=30

# Extra key=value pairs added to request (comma-separated), e.g.:
# VLM_API_EXTRA_FIELDS=lang=zh,temperature=0.2
VLM_API_EXTRA_FIELDS=

# Chat-completions style (e.g., SiliconFlow/OpenAI-compatible). When enabled, the backend will send
# JSON to /chat/completions with a text prompt + an image as a base64 data URL in messages[].
# Set VLM_API_URL to https://api.siliconflow.cn/v1/chat/completions for SiliconFlow.
VLM_API_CHAT=0
# Prompt used in the user message before the image part
VLM_API_PROMPT=Please describe the image concisely.
# Max tokens for the chat completion
VLM_API_MAX_TOKENS=512
VLM_API_TEMPERATURE=0.7
VLM_API_TOP_P=0.7

# Example (SiliconFlow):
# VLM_API_URL=https://api.siliconflow.cn/v1/chat/completions
# VLM_API_KEY=sk-xxxxxxxx
# VLM_API_CHAT=1
# VLM_API_MODEL=Qwen/Qwen3-VL-30B-A3B-Instruct
# VLM_API_PROMPT=用中文简洁描述这张图片的内容
# VLM_API_MAX_TOKENS=256

